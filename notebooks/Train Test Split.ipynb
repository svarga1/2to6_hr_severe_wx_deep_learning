{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bfaab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup_file: /home/samuel.varga/python_packages/WoF_post/wofs/data/psadilookup.dat\n"
     ]
    }
   ],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "from os.path import join, exists\n",
    "import joblib\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import sys, os\n",
    "import pyresample\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "#Filters\n",
    "from scipy.ndimage import uniform_filter, maximum_filter, gaussian_filter\n",
    "\n",
    "#Custom Packages\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post') #WoF post package\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "sys.path.append('/home/samuel.varga/projects/deep_learning/')\n",
    "\n",
    "from wofs.post.utils import (\n",
    "    save_dataset,\n",
    "    load_multiple_nc_files,\n",
    ")\n",
    "from main.dl_2to6_data_pipeline import get_files, load_dataset\n",
    "from collections import ChainMap\n",
    "\n",
    "#Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import shapely\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from wofs.plotting.wofs_colors import WoFSColors\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc31942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of Patch files - convert cases to datetime\n",
    "path_base = f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/'\n",
    "file_base = f'wofs_DL2TO6_16_16_data.feather'\n",
    "meta_file_base = f'wofs_DL2TO6_16_16_meta.feather'\n",
    "out_path = '/work/samuel.varga/data/DEEP_LEARNING/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c64c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[d for d in os.listdir(path_base) if '.txt' not in d]\n",
    "\n",
    "paths=[] #Valid paths for worker function\n",
    "bad_paths=[]\n",
    "for d in dates:\n",
    "    if d[4:6] !='05': \n",
    "        continue\n",
    "\n",
    "    times = [t for t in os.listdir(join(path_base, d)) if 'basemap' not in t] #Init time\n",
    "\n",
    "    for t in times:\n",
    "        path = join(path_base, d , t)\n",
    "        if exists(join(path,file_base)):\n",
    "            paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895acbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/20200518/1800\n",
      "Num Total Paths: 961 \n"
     ]
    }
   ],
   "source": [
    "print(paths[0])\n",
    "print(f'Num Total Paths: {len(paths)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd95296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Bad path found\n",
      "Num Paths w/ no Missing data: 949\n"
     ]
    }
   ],
   "source": [
    "#Check files to see where bad MRMS data, drop cases from list of files\n",
    "for path in paths:\n",
    "    ds = xr.load_dataset(join(join(path_base, path), file_base))\n",
    "    if np.any(ds['MESH_severe__4km'].values<0) or np.any(ds['MRMS_DZ'].values<0):\n",
    "        print('Bad path found')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "    ds.close()\n",
    "print(f'Num Paths w/ no Missing data: {len(paths)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c58712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20190501', '20190502', '20190503', '20190506', '20190507',\n",
       "       '20190508', '20190509', '20190510', '20190513', '20190514',\n",
       "       '20190515', '20190516', '20190517', '20190518', '20190520',\n",
       "       '20190521', '20190522', '20190523', '20190524', '20190526',\n",
       "       '20190528', '20190529', '20190530', '20200501', '20200504',\n",
       "       '20200505', '20200506', '20200507', '20200508', '20200513',\n",
       "       '20200515', '20200518', '20200519', '20200520', '20200521',\n",
       "       '20200522', '20200526', '20200527', '20200528', '20200529',\n",
       "       '20210503', '20210504', '20210505', '20210507', '20210510',\n",
       "       '20210513', '20210514', '20210517', '20210519', '20210521',\n",
       "       '20210524', '20210525', '20210526', '20210528', '20220502',\n",
       "       '20220505', '20220506', '20220511', '20220512', '20220516',\n",
       "       '20220518', '20220520', '20220523', '20220524', '20220526',\n",
       "       '20220527', '20220529', '20220530', '20230501', '20230502',\n",
       "       '20230503', '20230504', '20230505', '20230508', '20230509',\n",
       "       '20230510', '20230511', '20230512', '20230515', '20230516',\n",
       "       '20230517', '20230518', '20230519', '20230521', '20230522',\n",
       "       '20230523', '20230524', '20230525', '20230526', '20230530',\n",
       "       '20230531'], dtype='<U8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert remaining files into train/validation/test based on day\n",
    "temp_paths=[path.split('/')[-2][0:8]+path.split('/')[-1] for path in paths] #Different domains on the same day are treated as identical for the purposes of T/T split\n",
    "dates=[pd.to_datetime(path, format=f'%Y%m%d%H%M') for path in temp_paths]\n",
    "np.unique([date.strftime('%Y%m%d') for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcb281ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dates:\n",
      "['20210525' '20200526' '20200515' '20220502' '20200529' '20190509'\n",
      " '20220529' '20230504' '20190521' '20230522' '20230509' '20200506'\n",
      " '20190520' '20190524' '20200504' '20200528' '20220511' '20200505'\n",
      " '20190503' '20210507' '20230523' '20190526' '20200518' '20230526'\n",
      " '20190513' '20210521' '20220524' '20230510' '20200501' '20190514'\n",
      " '20230516' '20190501' '20210528' '20230521' '20230517' '20220527'\n",
      " '20200507' '20190517' '20190522' '20190529' '20210524' '20190528'\n",
      " '20220526' '20210504' '20230501' '20190510' '20230511' '20210517'\n",
      " '20220518' '20210519' '20230525' '20200522' '20230518' '20230515'\n",
      " '20220523' '20190506' '20210514' '20190516' '20230530' '20210526'\n",
      " '20200519' '20210510' '20210505']\n",
      "Testing Dates:\n",
      "['20230531' '20190518' '20220506' '20190530' '20200520' '20190508'\n",
      " '20230503' '20190507' '20210503' '20230519' '20230524' '20210513'\n",
      " '20220516' '20190523' '20200521' '20220530' '20200527' '20200508'\n",
      " '20220505' '20200513' '20230512' '20230505' '20190502' '20220520'\n",
      " '20220512' '20230502' '20190515' '20230508']\n"
     ]
    }
   ],
   "source": [
    "#Split into train/test\n",
    "from sklearn.model_selection import KFold as kfold, train_test_split\n",
    "import random\n",
    "\n",
    "all_dates = np.unique([date.strftime('%Y%m%d') for date in dates])\n",
    "random.Random(42).shuffle(all_dates)\n",
    "train_dates, test_dates = train_test_split(all_dates, test_size=0.3)\n",
    "print('Training Dates:')\n",
    "print(train_dates)\n",
    "\n",
    "print('Testing Dates:')\n",
    "print(test_dates)\n",
    "\n",
    "#Split training set into 5 folds\n",
    "train_folds = kfold(n_splits = 5, random_state=42, shuffle=True).split(train_dates)\n",
    "\n",
    "#with open(f'/work/samuel.varga/data/dates_split_deep_learning.pkl', 'wb') as date_file:\n",
    "    #pickle.dump({'train_dates':train_dates,'test_dates':test_dates}, date_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154ee3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation: 0\n",
      "[ 1  2  3  4  6  7  8 10 11 13 14 15 17 18 19 20 21 22 23 24 26 27 28 29\n",
      " 30 31 32 33 34 35 37 38 39 41 42 44 45 46 47 48 49 50 51 52 53 54 56 58\n",
      " 59 62] [ 0  5  9 12 16 25 36 40 43 55 57 60 61]\n",
      "534\n",
      "129\n",
      "Rotation: 1\n",
      "[ 0  1  2  5  7  9 10 11 12 14 15 16 18 20 21 22 23 24 25 26 27 28 29 30\n",
      " 31 32 35 36 37 38 39 40 41 42 43 44 45 47 50 51 52 54 55 56 57 58 59 60\n",
      " 61 62] [ 3  4  6  8 13 17 19 33 34 46 48 49 53]\n",
      "521\n",
      "142\n",
      "Rotation: 2\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 16 17 18 19 20 21 22 23 25 28\n",
      " 29 33 34 35 36 37 38 39 40 42 43 44 46 47 48 49 50 51 52 53 55 56 57 60\n",
      " 61 62] [11 15 24 26 27 30 31 32 41 45 54 58 59]\n",
      "527\n",
      "136\n",
      "Rotation: 3\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 24 25 26 27\n",
      " 28 30 31 32 33 34 36 38 40 41 42 43 45 46 48 49 51 52 53 54 55 57 58 59\n",
      " 60 61 62] [ 1  2 21 23 29 35 37 39 44 47 50 56]\n",
      "539\n",
      "124\n",
      "Rotation: 4\n",
      "[ 0  1  2  3  4  5  6  8  9 11 12 13 15 16 17 19 21 23 24 25 26 27 29 30\n",
      " 31 32 33 34 35 36 37 39 40 41 43 44 45 46 47 48 49 50 53 54 55 56 57 58\n",
      " 59 60 61] [ 7 10 14 18 20 22 28 38 42 51 52 62]\n",
      "531\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(train_folds):\n",
    "    print(f'Rotation: {i}')\n",
    "    print(train_index, val_index)\n",
    "    print(len(list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), train_dates[train_index])])))\n",
    "    print(len(list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), train_dates[val_index])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98dcecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rotation_nc(rot_num, train_ind, val_ind, unique_dates, path_list, date_list, filenames=None):\n",
    "    '''rot_num: int - rotation number\n",
    "        train_ind: list - list of indices for training folds - indices correspond to day in training_dates\n",
    "        val_ind: list - list of indices for validation folds - indices correspond to day in training_dates\n",
    "        unique_dates: list - list of unique dates in training set\n",
    "        path_list: list - list of file paths of length N that contain directory info and init time\n",
    "        date_list: list - list of dates of length N, with each date being YYYYmmdd for the corresponding path in path_list\n",
    "    '''\n",
    "    #Get list of paths for current rotation\n",
    "    training_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[train_ind])])\n",
    "    validation_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[val_ind])])\n",
    "    \n",
    "    #Add the filename to each of the paths\n",
    "    print('Appending Filenames')\n",
    "    training_file_paths = [join(path, file_base) for path in training_paths[:10]]\n",
    "    training_meta_paths=[join(path, meta_file_base) for path in training_paths[:10]]\n",
    "    validation_file_paths = [join(path, file_base) for path in validation_paths[:10]]\n",
    "    validation_meta_paths=[join(path, meta_file_base) for path in validation_paths[:10]]\n",
    "    \n",
    "    \n",
    "    #Create Training Data\n",
    "    print(f'Saving training data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in training_file_paths]\n",
    "    ds = xr.concat(ds, dim='patch_no')\n",
    "    #Save mean/variance for use in scaling <-----\n",
    "    #ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_data'))\n",
    "    return ds\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in training_meta_paths]\n",
    "    meta_ds = xr.concat(meta_ds)\n",
    "    #meta_ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_meta'))\n",
    "    meta_ds.close()\n",
    "    \n",
    "    #Create validation data\n",
    "    print(f'Saving validation data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in validation_file_paths]\n",
    "    ds = xr.concat(datasets)\n",
    "    #ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in validation_meta_paths]\n",
    "    meta_ds = xr.concat(meta_ds)\n",
    "   # meta_ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_meta'))\n",
    "    meta_ds.close()\n",
    "                          \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a8052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending Filenames\n",
      "Saving training data for Rot 0\n",
      "Appending Filenames\n",
      "Saving training data for Rot 1\n",
      "Appending Filenames\n",
      "Saving training data for Rot 2\n",
      "Appending Filenames\n",
      "Saving training data for Rot 3\n",
      "Appending Filenames\n",
      "Saving training data for Rot 4\n"
     ]
    }
   ],
   "source": [
    "#Save training folds:\n",
    "for i, (train_ind, val_ind) in enumerate(train_folds):\n",
    "    d = save_rotation_nc(i, train_ind, val_ind, train_dates, paths, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbedd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save testing set\n",
    "#save_rotation_nc(rotation, paths, dates, test_dates, ('wofs_dl_severe__2to6hr__test_data','wofs_dl_severe__2to6hr__test_meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename directory to data_workflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
