{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9bfaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "from os.path import join, exists\n",
    "import joblib\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import sys, os\n",
    "import pyresample\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "#Filters\n",
    "from scipy.ndimage import uniform_filter, maximum_filter, gaussian_filter\n",
    "\n",
    "#Custom Packages\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post') #WoF post package\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "sys.path.append('/home/samuel.varga/projects/deep_learning/')\n",
    "\n",
    "from wofs.post.utils import (\n",
    "    save_dataset,\n",
    "    load_multiple_nc_files,\n",
    ")\n",
    "from main.dl_2to6_data_pipeline import get_files, load_dataset\n",
    "from collections import ChainMap\n",
    "\n",
    "#Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import shapely\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from wofs.plotting.wofs_colors import WoFSColors\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc31942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of Patch files - convert cases to datetime\n",
    "path_base = f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/'\n",
    "file_base = f'wofs_DL2TO6_16_16_data.feather'\n",
    "meta_file_base = f'wofs_DL2TO6_16_16_meta.feather'\n",
    "out_path = '/work/samuel.varga/data/DEEP_LEARNING/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c64c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=[d for d in os.listdir(path_base) if '.txt' not in d]\n",
    "\n",
    "paths=[] #Valid paths for worker function\n",
    "bad_paths=[]\n",
    "for d in dates:\n",
    "    if d[4:6] !='05': \n",
    "        continue\n",
    "\n",
    "    times = [t for t in os.listdir(join(path_base, d)) if 'basemap' not in t] #Init time\n",
    "\n",
    "    for t in times:\n",
    "        path = join(path_base, d , t)\n",
    "        if exists(join(path,file_base)):\n",
    "            paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "895acbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/20180510/1900\n",
      "Num Total Paths: 673 \n"
     ]
    }
   ],
   "source": [
    "print(paths[0])\n",
    "print(f'Num Total Paths: {len(paths)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd95296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check files to see where bad MRMS data, drop cases from list of files\n",
    "for path in paths:\n",
    "    ds = xr.load_dataset(join(join(path_base, path), file_base))\n",
    "    if np.any(ds['MESH_severe__4km'].values<0) or np.any(ds['MRMS_DZ'].values<0):\n",
    "        print('Bad path found')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "print(f'Num Paths w/ no Missing data: {len(paths)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15c58712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20180501', '20180502', '20180503', '20180504', '20180507',\n",
       "       '20180509', '20180510', '20180511', '20180512', '20180514',\n",
       "       '20180515', '20180516', '20180519', '20180521', '20180523',\n",
       "       '20180524', '20180525', '20180527', '20180528', '20180529',\n",
       "       '20180530', '20180531', '20190501', '20190502', '20190503',\n",
       "       '20190506', '20190507', '20190508', '20190509', '20190510',\n",
       "       '20190513', '20190514', '20190515', '20190516', '20190517',\n",
       "       '20190518', '20190520', '20190521', '20190522', '20190523',\n",
       "       '20190524', '20190526', '20190528', '20190529', '20190530',\n",
       "       '20200501', '20200504', '20200505', '20200506', '20200507',\n",
       "       '20200508', '20200513', '20200515', '20200518', '20200519',\n",
       "       '20200520', '20200521', '20200522', '20200526', '20200527',\n",
       "       '20200528', '20200529', '20230501', '20230502', '20230503',\n",
       "       '20230504', '20230505', '20230508', '20230509', '20230510',\n",
       "       '20230511', '20230512', '20230515', '20230516', '20230517',\n",
       "       '20230518', '20230519', '20230521', '20230522', '20230523',\n",
       "       '20230524', '20230525', '20230526', '20230530', '20230531'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert remaining files into train/validation/test based on day\n",
    "temp_paths=[path.split('/')[-2][0:8]+path.split('/')[-1] for path in paths] #Different domains on the same day are treated as identical for the purposes of T/T split\n",
    "dates=[pd.to_datetime(path, format=f'%Y%m%d%H%M') for path in temp_paths]\n",
    "np.unique([date.strftime('%Y%m%d') for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb281ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train/test\n",
    "from sklearn.model_selection import KFold as kfold\n",
    "\n",
    "all_dates = np.unique([date.strftime('%Y%m%d') for date in dates])\n",
    "random.Random(42).shuffle(all_dates)\n",
    "train_dates, test_dates = train_test_split(all_dates, test_size=0.3)\n",
    "print(test_dates)\n",
    "\n",
    "#Split training set into 5 folds\n",
    "train_folds = kfold(n_splits = 5, random_state=42).split(train_dates)\n",
    "\n",
    "with open(f'/work/samuel.varga/data/dates_split_deep_learning.pkl', 'wb') as date_file:\n",
    "    pickle.dump({'train_dates':train_dates,'test_dates':test_dates}, date_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rotation_nc(rot_num, path_list, date_list, rot_dates, filenames=None):\n",
    "    #Get list of paths for current rotation\n",
    "    rotation_paths=path_list[np.array([date.strftime('%Y%m%d') for date in date_list]).isin(rot_dates)] \n",
    "    \n",
    "    #Add the filename to each of the paths\n",
    "    print('Appending Filename')\n",
    "    rotation_file_paths = [join(path, file_base) for path in rotation_paths]\n",
    "    rotation_meta_paths=[join(path, meta_file_base) for path in rotation_paths]\n",
    "    \n",
    "    \n",
    "    #Open and concat the datasets, then save\n",
    "    print(f'Saving ML Data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in rotation_file_paths]\n",
    "    ds = xr.concat(datasets)\n",
    "    ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in meta_file_paths]\n",
    "    meta_ds = xr.concat(meta_ds)\n",
    "    meta_ds.to_netcdf(join(out_path, filenames[0] if filenames else f'wofs_dl_severe__2to6hr__rot_{rot_num}__meta'))\n",
    "    meta_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training folds:\n",
    "for rotation in enumerate([train_folds]):\n",
    "    save_rotation_nc(rotation, paths, dates, train_folds[rotation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbedd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save testing set\n",
    "save_rotation_nc(rotation, paths, dates, test_dates, ('wofs_dl_severe__2to6hr__test__data','wofs_dl_severe__2to6hr__test__meta'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
