{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bfaab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup_file: /home/samuel.varga/python_packages/WoF_post/wofs/data/psadilookup.dat\n"
     ]
    }
   ],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "from os.path import join, exists\n",
    "import joblib\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import sys, os\n",
    "import pyresample\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "#Filters\n",
    "from scipy.ndimage import uniform_filter, maximum_filter, gaussian_filter\n",
    "\n",
    "#Custom Packages\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post') #WoF post package\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "sys.path.append('/home/samuel.varga/projects/deep_learning/')\n",
    "\n",
    "from wofs.post.utils import (\n",
    "    save_dataset,\n",
    "    load_multiple_nc_files,\n",
    ")\n",
    "from data_utils.dl_2to6_data_pipeline import get_files, load_dataset\n",
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc31942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of Patch files - convert cases to datetime\n",
    "path_base = f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/'\n",
    "file_base = f'wofs_DL2TO6_16_16_data.feather'\n",
    "meta_file_base = f'wofs_DL2TO6_16_16_meta.feather'\n",
    "out_path = '/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40893fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/20200518/1800\n",
      "Num Total Paths: 1154 \n"
     ]
    }
   ],
   "source": [
    "dates=[d for d in os.listdir(path_base) if '.txt' not in d]\n",
    "\n",
    "paths=[] #Valid paths for worker function\n",
    "bad_paths=[]\n",
    "for d in dates:\n",
    "    if d[4:6] !='05': \n",
    "        continue\n",
    "\n",
    "    times = [t for t in os.listdir(join(path_base, d)) if 'basemap' not in t] #Init time\n",
    "\n",
    "    for t in times:\n",
    "        path = join(path_base, d , t)\n",
    "        if exists(join(path,file_base)):\n",
    "            paths.append(path)\n",
    "print(paths[0])\n",
    "print(f'Num Total Paths: {len(paths)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd95296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad path found - MRMS DZ Values exceed expected range\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Num Paths w/ usable data: 1129\n"
     ]
    }
   ],
   "source": [
    "#Check files to see where bad MRMS data, drop cases from list of files\n",
    "for path in paths:\n",
    "    ds = xr.load_dataset(join(join(path_base, path), file_base))\n",
    "    if np.any(ds['MESH_severe__4km'].values<0) or np.any(ds['MRMS_DZ'].values<0):\n",
    "        print('Bad path found - Missing Data')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "    elif np.any(ds['MRMS_DZ'].values > 10**35):\n",
    "        print('Bad path found - MRMS DZ Values exceed expected range')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "    ds.close()\n",
    "print(f'Num Paths w/ usable data: {len(paths)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c58712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dates:\n",
      "['20200522' '20230523' '20210507' '20190503' '20230511' '20230512'\n",
      " '20230510' '20230515' '20230522' '20190509' '20210513' '20220513'\n",
      " '20200527' '20230530' '20190517' '20200508' '20210526' '20190530'\n",
      " '20210521' '20190515' '20190502' '20220518' '20210519' '20230519'\n",
      " '20230508' '20220503' '20230525' '20190516' '20220526' '20220529'\n",
      " '20220509' '20190525' '20220523' '20210512' '20210517' '20210527'\n",
      " '20230521' '20190528' '20210510' '20190522' '20220525' '20200515'\n",
      " '20200520' '20220512' '20220528' '20230501' '20210504' '20210525'\n",
      " '20220520' '20230531' '20190513' '20230502' '20190510' '20190507'\n",
      " '20220504' '20190524' '20190506' '20210528' '20200505' '20230524'\n",
      " '20200529' '20190518' '20190529' '20220524' '20210518' '20220511'\n",
      " '20190523' '20190521' '20200519' '20210520' '20200506' '20190514'\n",
      " '20230518' '20220506' '20200501']\n",
      "Testing Dates:\n",
      "['20200528' '20200507' '20230503' '20220519' '20210523' '20190501'\n",
      " '20210505' '20230505' '20200526' '20200521' '20210503' '20220527'\n",
      " '20230509' '20220516' '20220510' '20230517' '20210514' '20230516'\n",
      " '20190526' '20220517' '20220502' '20200504' '20210524' '20220505'\n",
      " '20190520' '20200513' '20200518' '20230526' '20230504' '20190508'\n",
      " '20220530' '20210506' '20220531']\n"
     ]
    }
   ],
   "source": [
    "#Convert remaining files into train/validation/test based on day\n",
    "temp_paths=[path.split('/')[-2][0:8]+path.split('/')[-1] for path in paths] #Different domains on the same day are treated as identical for the purposes of T/T split\n",
    "dates=[pd.to_datetime(path, format=f'%Y%m%d%H%M') for path in temp_paths]\n",
    "\n",
    "#Split into train/test\n",
    "from sklearn.model_selection import KFold as kfold, train_test_split\n",
    "import random\n",
    "\n",
    "all_dates = np.unique([date.strftime('%Y%m%d') for date in dates])\n",
    "random.Random(42).shuffle(all_dates)\n",
    "train_dates, test_dates = train_test_split(all_dates, test_size=0.3)\n",
    "print('Training Dates:')\n",
    "print(train_dates)\n",
    "\n",
    "print('Testing Dates:')\n",
    "print(test_dates)\n",
    "\n",
    "#with open(f'/work/samuel.varga/data/dates_split_deep_learning.pkl', 'wb') as date_file:\n",
    "#    pickle.dump({'train_dates':train_dates,'test_dates':test_dates}, date_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50cefab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/work/samuel.varga/data/dates_split_deep_learning.pkl', 'rb') as date_file:\n",
    "    d = pickle.load(date_file)\n",
    "train_dates, test_dates = d['train_dates'], d['test_dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9beef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training set into 5 folds\n",
    "train_folds = kfold(n_splits = 5, random_state=42, shuffle=True).split(train_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab880e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata(meta_data_list):\n",
    "    '''Reformats the metadata to appease the duplicate index errors.'''\n",
    "    '''Args: meta_data_list: list of opened datasets'''\n",
    "    meta = {}\n",
    "    for v in meta_data_list[0].variables:\n",
    "        #print(v)\n",
    "        if v in ['run_date','init_time','patch_no']:\n",
    "            meta[v] = np.append(np.array([]), [ x[v].values for x in meta_data_list])\n",
    "        else:\n",
    "            meta[v] = (['patch','NY_ind','NX_ind'],np.reshape(np.append(np.array([]), [x[v].values for x in meta_data_list]), (10*len(meta_data_list),16,16)))\n",
    "        #print(np.shape(meta[v]))\n",
    "\n",
    "    #Open NC file, add vars, save\n",
    "    meta_ds = xr.Dataset(meta)\n",
    "    return meta_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d270d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rotation_nc(rot_num, train_ind, val_ind, unique_dates, path_list, date_list, out_path=out_path):\n",
    "    '''rot_num: int - rotation number\n",
    "        train_ind: list - list of indices for training folds - indices correspond to day in training_dates\n",
    "        val_ind: list - list of indices for validation folds - indices correspond to day in training_dates\n",
    "        unique_dates: list - list of unique dates in training set\n",
    "        path_list: list - list of file paths of length N that contain directory info and init time\n",
    "        date_list: list - list of dates of length N, with each date being YYYYmmdd for the corresponding path in path_list\n",
    "    '''\n",
    "    #Get list of paths for current rotation\n",
    "    training_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[train_ind])])\n",
    "    validation_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[val_ind])])\n",
    "    \n",
    "    #Add the filename to each of the paths\n",
    "    print('Appending Filenames')\n",
    "    training_file_paths = [join(path, file_base) for path in training_paths[:10]]\n",
    "    training_meta_paths=[join(path, meta_file_base) for path in training_paths[:10]]\n",
    "    validation_file_paths = [join(path, file_base) for path in validation_paths[:10]]\n",
    "    validation_meta_paths=[join(path, meta_file_base) for path in validation_paths[:10]]\n",
    "    \n",
    "    \n",
    "    #Create Training Data\n",
    "    print(f'Saving training data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in training_file_paths]\n",
    "    ds = xr.concat(ds, dim='patch_no')\n",
    "    \n",
    "    #Save mean/variance for use in scaling \n",
    "    mean = np.array([np.nanmean(ds[v]) for v in ds.variables if 'severe' not in v])\n",
    "    var = np.array([np.nanvar(ds[v]) for v in ds.variables if 'severe' not in v])\n",
    "    with open(f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/scaling/rot_{rot_num}_scaling.pkl', 'wb') as scale_file:\n",
    "        pickle.dump({'mean':mean,'var':var}, scale_file)\n",
    "    \n",
    "    ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in training_meta_paths]\n",
    "    meta_ds = format_metadata(meta_ds)\n",
    "    meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_meta'))\n",
    "    meta_ds.close()\n",
    "    \n",
    "    #Create validation data\n",
    "    print(f'Saving validation data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in validation_file_paths]\n",
    "    ds = xr.concat(ds, dim='patch_no')\n",
    "    ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in validation_meta_paths]\n",
    "    meta_ds = format_metadata(meta_ds)\n",
    "    meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_meta'))\n",
    "    meta_ds.close()\n",
    "                          \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a8052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending Filenames\n",
      "Saving training data for Rot 0\n",
      "Saving metadata for Rot 0\n",
      "Saving validation data for Rot 0\n",
      "Saving metadata for Rot 0\n",
      "Appending Filenames\n",
      "Saving training data for Rot 1\n",
      "Saving metadata for Rot 1\n",
      "Saving validation data for Rot 1\n",
      "Saving metadata for Rot 1\n",
      "Appending Filenames\n",
      "Saving training data for Rot 2\n",
      "Saving metadata for Rot 2\n",
      "Saving validation data for Rot 2\n",
      "Saving metadata for Rot 2\n",
      "Appending Filenames\n",
      "Saving training data for Rot 3\n",
      "Saving metadata for Rot 3\n",
      "Saving validation data for Rot 3\n",
      "Saving metadata for Rot 3\n",
      "Appending Filenames\n",
      "Saving training data for Rot 4\n",
      "Saving metadata for Rot 4\n",
      "Saving validation data for Rot 4\n",
      "Saving metadata for Rot 4\n"
     ]
    }
   ],
   "source": [
    "#Save training folds:\n",
    "for i, (train_ind, val_ind) in enumerate(train_folds):\n",
    "    d = save_rotation_nc(i, train_ind, val_ind, train_dates, paths, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcbedd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving testing data\n",
      "Saving testing metadata\n"
     ]
    }
   ],
   "source": [
    "#Save testing set\n",
    "testing_paths=list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), test_dates)])\n",
    "testing_file_paths = [join(path, file_base) for path in testing_paths[:10]]\n",
    "testing_meta_paths=[join(path, meta_file_base) for path in testing_paths[:10]]\n",
    "\n",
    "\n",
    "print(f'Saving testing data')\n",
    "ds = [xr.open_dataset(f) for f in testing_file_paths]\n",
    "ds = xr.concat(ds, dim='patch_no')\n",
    "#ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__testing_data'))\n",
    "ds.close()\n",
    "    \n",
    "print(f'Saving testing metadata')\n",
    "meta_ds = [xr.open_dataset(f) for f in testing_meta_paths]\n",
    "meta_ds = format_metadata(meta_ds)\n",
    "#meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__testing_meta'))\n",
    "meta_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c741d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rotation(filepath, rotation, target_column, training=False, verbose=False):\n",
    "    '''Loads the rotation file, reshapes to be (samples, y, x, channels), selects appropriate target variables,\n",
    "    and returns the predictors and targets as arrays'''\n",
    "    '''Arguments:\n",
    "    filepath - path to nc file \n",
    "    rotation - int - rotation number\n",
    "    training - boolean - if true, returns scalers as well as data '''\n",
    "    \n",
    "    #Load Scaling information if loading training data\n",
    "    if 'train' in filepath or training:\n",
    "        training=True\n",
    "        print('Training path detected - loading scaling')\n",
    "        scalers = pd.read_pickle(f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/scaling/rot_{rotation}_scaling.pkl')\n",
    "        predictor_mean, predictor_variance = scalers['mean'], scalers['var']\n",
    "        \n",
    "    \n",
    "    #Load NCDF\n",
    "    ds = xr.open_dataset(filepath, engine='netcdf4')\n",
    "    \n",
    "    #Split predictors and targets and reshape into (samples, lat, lon, channels)\n",
    "    X = np.stack([ds[v].values for v in ds.variables if 'severe' not in v], axis=-1)\n",
    "    y = np.stack([ds[v].values for v in ds.variables if 'severe' in v], axis=-1)\n",
    "    \n",
    "    #Select specified target variable\n",
    "    target_ind = np.argwhere(np.array([v for v in ds.variables if 'severe' in v])==target_column)[0][0]\n",
    "    y = y[:,:,:,target_ind]\n",
    "    \n",
    "    #Debug\n",
    "    if verbose:\n",
    "        print(targ_ind)\n",
    "        print(np.shape(X))\n",
    "        print(np.shape(y))\n",
    "        \n",
    "    ds.close()\n",
    "    \n",
    "    if training:\n",
    "        return X, y, predictor_mean, predictor_variance\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb2319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y,  = load_rotation(join('/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/','wofs_dl_severe__2to6hr__rot_4__validation_data'), 4, 'any_severe__36km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a15b4594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 16, 16, 63)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fdaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
