{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bfaab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/samuel.varga/python_packages/MontePython/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/samuel.varga/projects/deep_learning/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwofs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     save_dataset,\n\u001b[1;32m     28\u001b[0m     load_multiple_nc_files,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdl_2to6_data_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_files, load_dataset\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChainMap\n",
      "File \u001b[0;32m~/python_packages/WoF_post/wofs/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_environment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_environment\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_per_member\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_per_member\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_paintball\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_paintball\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_asos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_asos\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting_scripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_storm_reports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_storm_reports\n",
      "File \u001b[0;32m~/python_packages/WoF_post/wofs/plotting/plotting_scripts/plot_paintball.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwofs_plotter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WoFSPlotter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPreProcessor\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpost\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwofs_cbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m identify_deterministic_tracks\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_config, check_file_type, check_threshs_for_plot_type, decompose_file_path\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_paintball\u001b[39m(var, file, outdir, timestep_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, plot_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msevere\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                   filename_suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/python_packages/WoF_post/wofs/post/wofs_cbook.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_ordered_dict\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# WoFS-based module for object identification.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmonte_python\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m################  Constants ################\u001b[39;00m\n\u001b[1;32m     23\u001b[0m e_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6.1173\u001b[39m \t\t\t\u001b[38;5;66;03m#std atm vapor pressure (hPa)\u001b[39;00m\n",
      "File \u001b[0;32m~/python_packages/MontePython/monte_python/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Handle importing to make package more user-friendly \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_identification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label, quantize_probabilities \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_quality_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QualityControler \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObjectMatcher\n",
      "File \u001b[0;32m~/python_packages/MontePython/monte_python/object_identification.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObjectMatcher\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_quality_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QualityControler\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mEnhancedWatershedSegmenter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EnhancedWatershed, rescale_data\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlabel\u001b[39m(input_data, params, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatershed\u001b[39m\u001b[38;5;124m'\u001b[39m, return_object_properties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/python_packages/MontePython/monte_python/object_quality_control.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeasure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regionprops \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KDTree\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#import warnings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#warnings.simplefilter(\"ignore\", UserWarning)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "from os.path import join, exists\n",
    "import joblib\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import sys, os\n",
    "import pyresample\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "#Filters\n",
    "from scipy.ndimage import uniform_filter, maximum_filter, gaussian_filter\n",
    "\n",
    "#Custom Packages\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post') #WoF post package\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "sys.path.append('/home/samuel.varga/projects/deep_learning/')\n",
    "\n",
    "from wofs.post.utils import (\n",
    "    save_dataset,\n",
    "    load_multiple_nc_files,\n",
    ")\n",
    "from data_utils.dl_2to6_data_pipeline import get_files, load_dataset\n",
    "from collections import ChainMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc31942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of Patch files - convert cases to datetime\n",
    "path_base = f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/'\n",
    "file_base = f'wofs_DL2TO6_16_16_data.feather'\n",
    "meta_file_base = f'wofs_DL2TO6_16_16_meta.feather'\n",
    "out_path = '/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40893fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/SummaryFiles/20200518/1800\n",
      "Num Total Paths: 1154 \n"
     ]
    }
   ],
   "source": [
    "dates=[d for d in os.listdir(path_base) if '.txt' not in d]\n",
    "\n",
    "paths=[] #Valid paths for worker function\n",
    "bad_paths=[]\n",
    "for d in dates:\n",
    "    if d[4:6] !='05': \n",
    "        continue\n",
    "\n",
    "    times = [t for t in os.listdir(join(path_base, d)) if 'basemap' not in t] #Init time\n",
    "\n",
    "    for t in times:\n",
    "        path = join(path_base, d , t)\n",
    "        if exists(join(path,file_base)):\n",
    "            paths.append(path)\n",
    "print(paths[0])\n",
    "print(f'Num Total Paths: {len(paths)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd95296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad path found - MRMS DZ Values exceed expected range\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Bad path found - Missing Data\n",
      "Num Paths w/ usable data: 1129\n"
     ]
    }
   ],
   "source": [
    "#Check files to see where bad MRMS data, drop cases from list of files\n",
    "for path in paths:\n",
    "    ds = xr.load_dataset(join(join(path_base, path), file_base))\n",
    "    if np.any(ds['MESH_severe__4km'].values<0) or np.any(ds['MRMS_DZ'].values<0):\n",
    "        print('Bad path found - Missing Data')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "    elif np.any(ds['MRMS_DZ'].values > 10**35):\n",
    "        print('Bad path found - MRMS DZ Values exceed expected range')\n",
    "        bad_paths.append(path)\n",
    "        paths.remove(path)\n",
    "    ds.close()\n",
    "print(f'Num Paths w/ usable data: {len(paths)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c58712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dates:\n",
      "['20210513' '20200526' '20220511' '20190525' '20200519' '20220509'\n",
      " '20210514' '20190530' '20210507' '20220526' '20210518' '20210520'\n",
      " '20190502' '20200529' '20210503' '20210510' '20190517' '20210504'\n",
      " '20230517' '20200501' '20230502' '20220524' '20230508' '20220529'\n",
      " '20200528' '20220519' '20230510' '20220525' '20200504' '20230516'\n",
      " '20190516' '20230526' '20190506' '20210524' '20230525' '20200520'\n",
      " '20210512' '20220518' '20220528' '20230512' '20220504' '20220520'\n",
      " '20220523' '20220512' '20220516' '20220530' '20220506' '20230523'\n",
      " '20230518' '20190523' '20190528' '20230503' '20230521' '20230531'\n",
      " '20190520' '20190510' '20190509' '20220531' '20200515' '20190526'\n",
      " '20230524' '20190508' '20210523' '20220510' '20230530' '20210517'\n",
      " '20220527' '20190513' '20190522' '20210528' '20220517' '20220502'\n",
      " '20200513' '20220503' '20190524']\n",
      "Testing Dates:\n",
      "['20190514' '20190503' '20230504' '20230501' '20200508' '20210505'\n",
      " '20200506' '20230522' '20190529' '20210506' '20190518' '20230505'\n",
      " '20220513' '20230515' '20210521' '20190515' '20200522' '20230519'\n",
      " '20200527' '20200521' '20210527' '20200507' '20200518' '20190501'\n",
      " '20210526' '20230511' '20220505' '20210519' '20210525' '20190507'\n",
      " '20200505' '20190521' '20230509']\n"
     ]
    }
   ],
   "source": [
    "#Convert remaining files into train/validation/test based on day\n",
    "temp_paths=[path.split('/')[-2][0:8]+path.split('/')[-1] for path in paths] #Different domains on the same day are treated as identical for the purposes of T/T split\n",
    "dates=[pd.to_datetime(path, format=f'%Y%m%d%H%M') for path in temp_paths]\n",
    "\n",
    "#Split into train/test\n",
    "from sklearn.model_selection import KFold as kfold, train_test_split\n",
    "import random\n",
    "\n",
    "all_dates = np.unique([date.strftime('%Y%m%d') for date in dates])\n",
    "random.Random(42).shuffle(all_dates)\n",
    "train_dates, test_dates = train_test_split(all_dates, test_size=0.3)\n",
    "print('Training Dates:')\n",
    "print(train_dates)\n",
    "\n",
    "print('Testing Dates:')\n",
    "print(test_dates)\n",
    "\n",
    "#Split training set into 5 folds\n",
    "train_folds = kfold(n_splits = 5, random_state=42, shuffle=True).split(train_dates)\n",
    "\n",
    "#with open(f'/work/samuel.varga/data/dates_split_deep_learning.pkl', 'wb') as date_file:\n",
    "#    pickle.dump({'train_dates':train_dates,'test_dates':test_dates}, date_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154ee3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (train_index, val_index) in enumerate(train_folds):\n",
    "#    print(f'Rotation: {i}')\n",
    "#    print(train_index, val_index)\n",
    "#    print(len(list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), train_dates[train_index])])))\n",
    "#    print(len(list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), train_dates[val_index])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab880e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata(meta_data_list):\n",
    "    '''Reformats the metadata to appease the duplicate index errors.'''\n",
    "    '''Args: meta_data_list: list of opened datasets'''\n",
    "    meta = {}\n",
    "    for v in meta_data_list[0].variables:\n",
    "        #print(v)\n",
    "        if v in ['run_date','init_time','patch_no']:\n",
    "            meta[v] = np.append(np.array([]), [ x[v].values for x in meta_data_list])\n",
    "        else:\n",
    "            meta[v] = (['patch','NY_ind','NX_ind'],np.reshape(np.append(np.array([]), [x[v].values for x in meta_data_list]), (10*len(meta_data_list),16,16)))\n",
    "        #print(np.shape(meta[v]))\n",
    "\n",
    "    #Open NC file, add vars, save\n",
    "    meta_ds = xr.Dataset(meta)\n",
    "    return meta_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d270d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rotation_nc(rot_num, train_ind, val_ind, unique_dates, path_list, date_list, out_path=out_path):\n",
    "    '''rot_num: int - rotation number\n",
    "        train_ind: list - list of indices for training folds - indices correspond to day in training_dates\n",
    "        val_ind: list - list of indices for validation folds - indices correspond to day in training_dates\n",
    "        unique_dates: list - list of unique dates in training set\n",
    "        path_list: list - list of file paths of length N that contain directory info and init time\n",
    "        date_list: list - list of dates of length N, with each date being YYYYmmdd for the corresponding path in path_list\n",
    "    '''\n",
    "    #Get list of paths for current rotation\n",
    "    training_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[train_ind])])\n",
    "    validation_paths=list(np.array(path_list)[np.isin(np.array([date.strftime('%Y%m%d') for date in date_list]), unique_dates[val_ind])])\n",
    "    \n",
    "    #Add the filename to each of the paths\n",
    "    print('Appending Filenames')\n",
    "    training_file_paths = [join(path, file_base) for path in training_paths[:10]]\n",
    "    training_meta_paths=[join(path, meta_file_base) for path in training_paths[:10]]\n",
    "    validation_file_paths = [join(path, file_base) for path in validation_paths[:10]]\n",
    "    validation_meta_paths=[join(path, meta_file_base) for path in validation_paths[:10]]\n",
    "    \n",
    "    \n",
    "    #Create Training Data\n",
    "    print(f'Saving training data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in training_file_paths]\n",
    "    ds = xr.concat(ds, dim='patch_no')\n",
    "    return ds\n",
    "    #Save mean/variance for use in scaling \n",
    "    mean = np.array([np.nanmean(ds[v]) for v in ds.variables if 'severe' not in v])\n",
    "    var = np.array([np.nanvar(ds[v]) for v in ds.variables if 'severe' not in v])\n",
    "    #with open(f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/scaling/rot_{rot_num}_scaling.pkl', 'wb') as scale_file:\n",
    "    #    pickle.dump({'mean':mean,'var':var}, scale_file)\n",
    "    \n",
    "    #ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in training_meta_paths]\n",
    "    meta_ds = format_metadata(meta_ds)\n",
    "    #meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__training_meta'))\n",
    "    meta_ds.close()\n",
    "    \n",
    "    #Create validation data\n",
    "    print(f'Saving validation data for Rot {rot_num}')\n",
    "    ds = [xr.open_dataset(f) for f in validation_file_paths]\n",
    "    ds = xr.concat(ds, dim='patch_no')\n",
    "    #ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_data'))\n",
    "    ds.close()\n",
    "    \n",
    "    print(f'Saving metadata for Rot {rot_num}')\n",
    "    meta_ds = [xr.open_dataset(f) for f in validation_meta_paths]\n",
    "    meta_ds = format_metadata(meta_ds)\n",
    "    #meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__rot_{rot_num}__validation_meta'))\n",
    "    meta_ds.close()\n",
    "                          \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a8052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending Filenames\n",
      "Saving training data for Rot 0\n",
      "Appending Filenames\n",
      "Saving training data for Rot 1\n",
      "Appending Filenames\n",
      "Saving training data for Rot 2\n",
      "Appending Filenames\n",
      "Saving training data for Rot 3\n",
      "Appending Filenames\n",
      "Saving training data for Rot 4\n"
     ]
    }
   ],
   "source": [
    "#Save training folds:\n",
    "for i, (train_ind, val_ind) in enumerate(train_folds):\n",
    "    d = save_rotation_nc(i, train_ind, val_ind, train_dates, paths, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbedd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving testing data\n",
      "Saving testing metadata\n"
     ]
    }
   ],
   "source": [
    "#Save testing set\n",
    "testing_paths=list(np.array(paths)[np.isin(np.array([date.strftime('%Y%m%d') for date in dates]), test_dates)])\n",
    "testing_file_paths = [join(path, file_base) for path in testing_paths[:10]]\n",
    "testing_meta_paths=[join(path, meta_file_base) for path in testing_paths[:10]]\n",
    "\n",
    "\n",
    "print(f'Saving testing data')\n",
    "ds = [xr.open_dataset(f) for f in testing_file_paths]\n",
    "ds = xr.concat(ds, dim='patch_no')\n",
    "#ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__testing_data'))\n",
    "ds.close()\n",
    "    \n",
    "print(f'Saving testing metadata')\n",
    "meta_ds = [xr.open_dataset(f) for f in testing_meta_paths]\n",
    "meta_ds = format_metadata(meta_ds)\n",
    "#meta_ds.to_netcdf(join(out_path, f'wofs_dl_severe__2to6hr__testing_meta'))\n",
    "meta_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c741d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rotation(filepath, rotation, target_column, training=False, verbose=False):\n",
    "    '''Loads the rotation file, reshapes to be (samples, y, x, channels), selects appropriate target variables,\n",
    "    and returns the predictors and targets as arrays'''\n",
    "    '''Arguments:\n",
    "    filepath - path to nc file \n",
    "    rotation - int - rotation number\n",
    "    training - boolean - if true, returns scalers as well as data '''\n",
    "    \n",
    "    #Load Scaling information if loading training data\n",
    "    if 'train' in filepath or training:\n",
    "        training=True\n",
    "        print('Training path detected - loading scaling')\n",
    "        scalers = pd.read_pickle(f'/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/scaling/rot_{rotation}_scaling.pkl')\n",
    "        predictor_mean, predictor_variance = scalers['mean'], scalers['var']\n",
    "        \n",
    "    \n",
    "    #Load NCDF\n",
    "    ds = xr.open_dataset(filepath, engine='netcdf4')\n",
    "    \n",
    "    #Split predictors and targets and reshape into (samples, lat, lon, channels)\n",
    "    X = np.stack([ds[v].values for v in ds.variables if 'severe' not in v], axis=-1)\n",
    "    y = np.stack([ds[v].values for v in ds.variables if 'severe' in v], axis=-1)\n",
    "    \n",
    "    #Select specified target variable\n",
    "    target_ind = np.argwhere(np.array([v for v in ds.variables if 'severe' in v])==target_column)[0][0]\n",
    "    y = y[:,:,:,target_ind]\n",
    "    \n",
    "    #Debug\n",
    "    if verbose:\n",
    "        print(targ_ind)\n",
    "        print(np.shape(X))\n",
    "        print(np.shape(y))\n",
    "        \n",
    "    ds.close()\n",
    "    \n",
    "    if training:\n",
    "        return X, y, predictor_mean, predictor_variance\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb2319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y,  = load_rotation(join('/work/samuel.varga/data/2to6_hr_severe_wx/DEEP_LEARNING/','wofs_dl_severe__2to6hr__rot_4__validation_data'), 4, 'any_severe__36km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "568fef63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b4594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
